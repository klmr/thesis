```{r echo=FALSE}
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(knitr, quietly = TRUE, warn.conflicts = FALSE)

options(stringsAsFactors = FALSE)

transform_counts = function (counts, ...)
    mutate_each_(counts, funs_(lazyeval::lazy_dots(...)), col_data$Sample)

summarize_each = summarise_each

panderOptions('table.split.table', Inf)
panderOptions('table.alignment.default',
              function (df) ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.alignment.rownames', 'left')

# Enable automatic table reformatting.
opts_chunk$set(render = function (object, ...) {
    if (is.data.frame(object) ||
        is.matrix(object) ||
        is.table(object) ||
        is.tbl_df(object))
        pander(object, style = 'rmarkdown')
    else if (isS4(object))
        show(object)
    else
        print(object)
})

pander.table = function (x, ...)
    pander(`rownames<-`(rbind(x), NULL), ...)

# Helpers for dplyr tables

is.tbl_df = function (x)
    'tbl_df' %in% class(x)

pander.tbl_df = function (x, ...)
    pander(trunc_mat(x), ...)

# Copied from dplyr:::print.trunc_mat
pander.trunc_mat = function (x, ...) {
    if (! is.null(x$table))
        pander(x$table, ...)

    if (length(x$extra) > 0) {
        var_types = paste0(names(x$extra), ' (', x$extra, ')', collapse = ', ')
        pander(dplyr:::wrap('Variables not shown: ', var_types))
    }
}

# Disable code re-formatting.
opts_chunk$set(tidy = FALSE)
```

We are going to use a small toy data set.

```{r}
experiment = data.frame(
    Gene = LETTERS[1 : 5],
    Length = c(1000, 1000 * (1 : 4)),
    lib1 = c(1000, 1000 * (1 : 4)),
    lib2 = c(1000, 1000 * (1 : 4)),
    lib3 = c(1000, 1000 * (1 : 4)),
    lib4 = c(2000, 1000 * (1 : 4))
)

col_data = data.frame(
    Sample = paste0('lib', 1 : 4),
    Condition = rep(c('control', 'treatment'), each = 2)
)
```

The `col_data` describes the *column data* of our experiment â€” in other words,
the experimental setup.

---

Here is how the data is transformed into FPKM. All calculations are performed in
log domain to avoid loss of precision in floating point calculations.

```{r}
fpkm = function (counts, transcript_lengths)
    exp(log(counts) - log(transcript_lengths) - log(sum(counts)) + log(1e9))

(fpkm_counts = transform_counts(experiment, fpkm(., Length)))
```

Here are TPM:

```{r}
tpm = function (counts, transcript_lengths) {
    log_by_size = log(counts) - log(transcript_lengths)
    exp(log_by_size - log(sum(exp(log_by_size))) + log(1E6))
}

(tpm_counts = transform_counts(experiment, tpm(., Length)))
```

And finally, library size factors. Unlike the previous methods, this uses data
across samples, and calculates per-sample normalisation factors.

```{r}
size_factors = function (samples) {
    log_samples = log(samples)
    log_means = rowMeans(log_samples)
    summarize_each(log_samples, funs(exp(median(. - log_means))))
}

(sf = size_factors(select(experiment, one_of(col_data$Sample))))
```
